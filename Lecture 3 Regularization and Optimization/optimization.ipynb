{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CAMKqyKF3o3i",
        "outputId": "1c2b5081-ed24-403d-fed4-3ab3db086d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in attempt 0 the loss was 8.999909, best 8.999909\n",
            "in attempt 1 the loss was 9.000082, best 8.999909\n",
            "in attempt 2 the loss was 8.999962, best 8.999909\n",
            "in attempt 3 the loss was 8.999946, best 8.999909\n",
            "in attempt 4 the loss was 9.000083, best 8.999909\n",
            "in attempt 5 the loss was 9.000090, best 8.999909\n",
            "in attempt 6 the loss was 8.999504, best 8.999504\n",
            "in attempt 7 the loss was 9.000169, best 8.999504\n",
            "in attempt 8 the loss was 8.999997, best 8.999504\n",
            "in attempt 9 the loss was 9.000102, best 8.999504\n",
            "in attempt 10 the loss was 8.999948, best 8.999504\n",
            "in attempt 11 the loss was 9.000279, best 8.999504\n",
            "in attempt 12 the loss was 8.999972, best 8.999504\n",
            "in attempt 13 the loss was 9.000424, best 8.999504\n",
            "in attempt 14 the loss was 8.999851, best 8.999504\n",
            "in attempt 15 the loss was 8.999956, best 8.999504\n",
            "in attempt 16 the loss was 9.000000, best 8.999504\n",
            "in attempt 17 the loss was 8.999867, best 8.999504\n",
            "in attempt 18 the loss was 9.000153, best 8.999504\n",
            "in attempt 19 the loss was 9.000007, best 8.999504\n",
            "in attempt 20 the loss was 8.999806, best 8.999504\n",
            "in attempt 21 the loss was 9.000187, best 8.999504\n",
            "in attempt 22 the loss was 8.999936, best 8.999504\n",
            "in attempt 23 the loss was 8.999909, best 8.999504\n",
            "in attempt 24 the loss was 9.000014, best 8.999504\n",
            "in attempt 25 the loss was 8.999535, best 8.999504\n",
            "in attempt 26 the loss was 8.999649, best 8.999504\n",
            "in attempt 27 the loss was 9.000282, best 8.999504\n",
            "in attempt 28 the loss was 9.000079, best 8.999504\n",
            "in attempt 29 the loss was 9.000315, best 8.999504\n",
            "in attempt 30 the loss was 9.000129, best 8.999504\n",
            "in attempt 31 the loss was 8.999387, best 8.999387\n",
            "in attempt 32 the loss was 8.999736, best 8.999387\n",
            "in attempt 33 the loss was 9.000073, best 8.999387\n",
            "in attempt 34 the loss was 9.000374, best 8.999387\n",
            "in attempt 35 the loss was 8.999883, best 8.999387\n",
            "in attempt 36 the loss was 9.000285, best 8.999387\n",
            "in attempt 37 the loss was 8.999647, best 8.999387\n",
            "in attempt 38 the loss was 8.999864, best 8.999387\n",
            "in attempt 39 the loss was 9.000052, best 8.999387\n",
            "in attempt 40 the loss was 9.000054, best 8.999387\n",
            "in attempt 41 the loss was 9.000047, best 8.999387\n",
            "in attempt 42 the loss was 9.000258, best 8.999387\n",
            "in attempt 43 the loss was 8.999943, best 8.999387\n",
            "in attempt 44 the loss was 9.000469, best 8.999387\n",
            "in attempt 45 the loss was 9.000252, best 8.999387\n",
            "in attempt 46 the loss was 9.000207, best 8.999387\n",
            "in attempt 47 the loss was 8.999934, best 8.999387\n",
            "in attempt 48 the loss was 9.000245, best 8.999387\n",
            "in attempt 49 the loss was 8.999612, best 8.999387\n",
            "in attempt 50 the loss was 9.000148, best 8.999387\n",
            "in attempt 51 the loss was 8.999873, best 8.999387\n",
            "in attempt 52 the loss was 9.000116, best 8.999387\n",
            "in attempt 53 the loss was 8.999677, best 8.999387\n",
            "in attempt 54 the loss was 9.000185, best 8.999387\n",
            "in attempt 55 the loss was 9.000231, best 8.999387\n",
            "in attempt 56 the loss was 9.000199, best 8.999387\n",
            "in attempt 57 the loss was 9.000125, best 8.999387\n",
            "in attempt 58 the loss was 8.999771, best 8.999387\n",
            "in attempt 59 the loss was 8.999834, best 8.999387\n",
            "in attempt 60 the loss was 8.999585, best 8.999387\n",
            "in attempt 61 the loss was 8.999948, best 8.999387\n",
            "in attempt 62 the loss was 8.999886, best 8.999387\n",
            "in attempt 63 the loss was 8.999990, best 8.999387\n",
            "in attempt 64 the loss was 8.999782, best 8.999387\n",
            "in attempt 65 the loss was 8.999515, best 8.999387\n",
            "in attempt 66 the loss was 9.000378, best 8.999387\n",
            "in attempt 67 the loss was 8.999798, best 8.999387\n",
            "in attempt 68 the loss was 8.999880, best 8.999387\n",
            "in attempt 69 the loss was 8.999585, best 8.999387\n",
            "in attempt 70 the loss was 8.999969, best 8.999387\n",
            "in attempt 71 the loss was 8.999897, best 8.999387\n",
            "in attempt 72 the loss was 8.999847, best 8.999387\n",
            "in attempt 73 the loss was 9.000077, best 8.999387\n",
            "in attempt 74 the loss was 9.000148, best 8.999387\n",
            "in attempt 75 the loss was 9.000063, best 8.999387\n",
            "in attempt 76 the loss was 9.000207, best 8.999387\n",
            "in attempt 77 the loss was 9.000101, best 8.999387\n",
            "in attempt 78 the loss was 8.999836, best 8.999387\n",
            "in attempt 79 the loss was 9.000001, best 8.999387\n",
            "in attempt 80 the loss was 9.000196, best 8.999387\n",
            "in attempt 81 the loss was 8.999970, best 8.999387\n",
            "in attempt 82 the loss was 8.999989, best 8.999387\n",
            "in attempt 83 the loss was 8.999845, best 8.999387\n",
            "in attempt 84 the loss was 9.000461, best 8.999387\n",
            "in attempt 85 the loss was 9.000415, best 8.999387\n",
            "in attempt 86 the loss was 9.000173, best 8.999387\n",
            "in attempt 87 the loss was 9.000179, best 8.999387\n",
            "in attempt 88 the loss was 9.000023, best 8.999387\n",
            "in attempt 89 the loss was 8.999856, best 8.999387\n",
            "in attempt 90 the loss was 8.999985, best 8.999387\n",
            "in attempt 91 the loss was 8.999992, best 8.999387\n",
            "in attempt 92 the loss was 9.000236, best 8.999387\n",
            "in attempt 93 the loss was 8.999990, best 8.999387\n",
            "in attempt 94 the loss was 9.000007, best 8.999387\n",
            "in attempt 95 the loss was 9.000018, best 8.999387\n",
            "in attempt 96 the loss was 8.999607, best 8.999387\n",
            "in attempt 97 the loss was 9.000198, best 8.999387\n",
            "in attempt 98 the loss was 9.000268, best 8.999387\n",
            "in attempt 99 the loss was 9.000103, best 8.999387\n",
            "in attempt 100 the loss was 8.999748, best 8.999387\n",
            "in attempt 101 the loss was 8.999739, best 8.999387\n",
            "in attempt 102 the loss was 9.000275, best 8.999387\n",
            "in attempt 103 the loss was 8.999600, best 8.999387\n",
            "in attempt 104 the loss was 8.999667, best 8.999387\n",
            "in attempt 105 the loss was 9.000237, best 8.999387\n",
            "in attempt 106 the loss was 8.999974, best 8.999387\n",
            "in attempt 107 the loss was 9.000066, best 8.999387\n",
            "in attempt 108 the loss was 9.000171, best 8.999387\n",
            "in attempt 109 the loss was 9.000368, best 8.999387\n",
            "in attempt 110 the loss was 9.000150, best 8.999387\n",
            "in attempt 111 the loss was 8.999865, best 8.999387\n",
            "in attempt 112 the loss was 9.000038, best 8.999387\n",
            "in attempt 113 the loss was 8.999957, best 8.999387\n",
            "in attempt 114 the loss was 9.000365, best 8.999387\n",
            "in attempt 115 the loss was 9.000058, best 8.999387\n",
            "in attempt 116 the loss was 8.999568, best 8.999387\n",
            "in attempt 117 the loss was 8.999889, best 8.999387\n",
            "in attempt 118 the loss was 8.999955, best 8.999387\n",
            "in attempt 119 the loss was 8.999943, best 8.999387\n",
            "in attempt 120 the loss was 9.000198, best 8.999387\n",
            "in attempt 121 the loss was 9.000273, best 8.999387\n",
            "in attempt 122 the loss was 8.999899, best 8.999387\n",
            "in attempt 123 the loss was 8.999946, best 8.999387\n",
            "in attempt 124 the loss was 8.999774, best 8.999387\n",
            "in attempt 125 the loss was 9.000054, best 8.999387\n",
            "in attempt 126 the loss was 9.000412, best 8.999387\n",
            "in attempt 127 the loss was 9.000500, best 8.999387\n",
            "in attempt 128 the loss was 8.999828, best 8.999387\n",
            "in attempt 129 the loss was 9.000170, best 8.999387\n",
            "in attempt 130 the loss was 8.999966, best 8.999387\n",
            "in attempt 131 the loss was 9.000283, best 8.999387\n",
            "in attempt 132 the loss was 8.999885, best 8.999387\n",
            "in attempt 133 the loss was 9.000100, best 8.999387\n",
            "in attempt 134 the loss was 8.999607, best 8.999387\n",
            "in attempt 135 the loss was 9.000293, best 8.999387\n",
            "in attempt 136 the loss was 9.000033, best 8.999387\n",
            "in attempt 137 the loss was 8.999807, best 8.999387\n",
            "in attempt 138 the loss was 9.000185, best 8.999387\n",
            "in attempt 139 the loss was 8.999094, best 8.999094\n",
            "in attempt 140 the loss was 9.000009, best 8.999094\n",
            "in attempt 141 the loss was 8.999825, best 8.999094\n",
            "in attempt 142 the loss was 8.999903, best 8.999094\n",
            "in attempt 143 the loss was 8.999974, best 8.999094\n",
            "in attempt 144 the loss was 8.999728, best 8.999094\n",
            "in attempt 145 the loss was 8.999771, best 8.999094\n",
            "in attempt 146 the loss was 9.000078, best 8.999094\n",
            "in attempt 147 the loss was 8.999918, best 8.999094\n",
            "in attempt 148 the loss was 9.000305, best 8.999094\n",
            "in attempt 149 the loss was 8.999861, best 8.999094\n",
            "in attempt 150 the loss was 8.999980, best 8.999094\n",
            "in attempt 151 the loss was 8.999569, best 8.999094\n",
            "in attempt 152 the loss was 8.999888, best 8.999094\n",
            "in attempt 153 the loss was 8.999807, best 8.999094\n",
            "in attempt 154 the loss was 9.000306, best 8.999094\n",
            "in attempt 155 the loss was 8.999758, best 8.999094\n",
            "in attempt 156 the loss was 9.000159, best 8.999094\n",
            "in attempt 157 the loss was 8.999744, best 8.999094\n",
            "in attempt 158 the loss was 8.999787, best 8.999094\n",
            "in attempt 159 the loss was 8.999890, best 8.999094\n",
            "in attempt 160 the loss was 9.000010, best 8.999094\n",
            "in attempt 161 the loss was 8.999856, best 8.999094\n",
            "in attempt 162 the loss was 8.999815, best 8.999094\n",
            "in attempt 163 the loss was 9.000270, best 8.999094\n",
            "in attempt 164 the loss was 9.000491, best 8.999094\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1644589204.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3073\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbestloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbestloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-1644589204.py\u001b[0m in \u001b[0;36mL\u001b[0;34m(X, Y, W)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Simulación de una función de pérdida SVM (sólo para prueba)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcorrect_class_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmargins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_class_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Supongamos que tenemos X_train y Y_train definidos\n",
        "# Aquí simplemente creamos datos de ejemplo aleatorios\n",
        "X_train = np.random.randn(3073, 50000)\n",
        "Y_train = np.random.randint(0, 10, size=(50000,))\n",
        "\n",
        "# Simulación de una función de pérdida SVM (sólo para prueba)\n",
        "def L(X, Y, W):\n",
        "    scores = W.dot(X)\n",
        "    correct_class_scores = scores[Y, np.arange(X.shape[1])]\n",
        "    margins = np.maximum(0, scores - correct_class_scores + 1)\n",
        "    margins[Y, np.arange(X.shape[1])] = 0\n",
        "    loss = np.sum(margins) / X.shape[1]\n",
        "    return loss\n",
        "\n",
        "# Strategy 1: Random Search\n",
        "bestloss = float(\"inf\")\n",
        "for num in range(1000):\n",
        "    W = np.random.randn(10, 3073) * 0.0001\n",
        "    loss = L(X_train, Y_train, W)\n",
        "    if loss < bestloss:\n",
        "        bestloss = loss\n",
        "        bestW = W\n",
        "    print(f'in attempt {num} the loss was {loss:.6f}, best {bestloss:.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCsWd87H4aSD",
        "outputId": "b94fbdaf-4673-4f58-e11b-cc45e7ec4d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0 loss is 8.995789\n",
            "iter 1 loss is 8.995789\n",
            "iter 2 loss is 8.995521\n",
            "iter 3 loss is 8.995086\n",
            "iter 4 loss is 8.995086\n",
            "iter 5 loss is 8.995086\n",
            "iter 6 loss is 8.994959\n",
            "iter 7 loss is 8.994959\n",
            "iter 8 loss is 8.994959\n",
            "iter 9 loss is 8.994959\n",
            "iter 10 loss is 8.994784\n",
            "iter 11 loss is 8.994784\n",
            "iter 12 loss is 8.994780\n",
            "iter 13 loss is 8.994780\n",
            "iter 14 loss is 8.994409\n",
            "iter 15 loss is 8.994355\n",
            "iter 16 loss is 8.994250\n",
            "iter 17 loss is 8.994250\n",
            "iter 18 loss is 8.994098\n",
            "iter 19 loss is 8.993852\n",
            "iter 20 loss is 8.993782\n",
            "iter 21 loss is 8.993782\n",
            "iter 22 loss is 8.993214\n",
            "iter 23 loss is 8.993214\n",
            "iter 24 loss is 8.992990\n",
            "iter 25 loss is 8.992919\n",
            "iter 26 loss is 8.992919\n",
            "iter 27 loss is 8.992483\n",
            "iter 28 loss is 8.992249\n",
            "iter 29 loss is 8.991964\n",
            "iter 30 loss is 8.991784\n",
            "iter 31 loss is 8.991784\n",
            "iter 32 loss is 8.991718\n",
            "iter 33 loss is 8.991718\n",
            "iter 34 loss is 8.991356\n",
            "iter 35 loss is 8.991104\n",
            "iter 36 loss is 8.991104\n",
            "iter 37 loss is 8.990875\n",
            "iter 38 loss is 8.990875\n",
            "iter 39 loss is 8.990683\n",
            "iter 40 loss is 8.990494\n",
            "iter 41 loss is 8.990251\n",
            "iter 42 loss is 8.989856\n",
            "iter 43 loss is 8.989776\n",
            "iter 44 loss is 8.989776\n",
            "iter 45 loss is 8.989776\n",
            "iter 46 loss is 8.989776\n",
            "iter 47 loss is 8.989723\n",
            "iter 48 loss is 8.989246\n",
            "iter 49 loss is 8.989115\n",
            "iter 50 loss is 8.988678\n",
            "iter 51 loss is 8.988622\n",
            "iter 52 loss is 8.988367\n",
            "iter 53 loss is 8.988367\n",
            "iter 54 loss is 8.988367\n",
            "iter 55 loss is 8.988110\n",
            "iter 56 loss is 8.987810\n",
            "iter 57 loss is 8.987442\n",
            "iter 58 loss is 8.987141\n",
            "iter 59 loss is 8.987077\n",
            "iter 60 loss is 8.987071\n",
            "iter 61 loss is 8.987071\n",
            "iter 62 loss is 8.987071\n",
            "iter 63 loss is 8.987071\n",
            "iter 64 loss is 8.987071\n",
            "iter 65 loss is 8.987071\n",
            "iter 66 loss is 8.987059\n",
            "iter 67 loss is 8.987038\n",
            "iter 68 loss is 8.987038\n",
            "iter 69 loss is 8.986640\n",
            "iter 70 loss is 8.986265\n",
            "iter 71 loss is 8.986094\n",
            "iter 72 loss is 8.986094\n",
            "iter 73 loss is 8.986007\n",
            "iter 74 loss is 8.985749\n",
            "iter 75 loss is 8.985749\n",
            "iter 76 loss is 8.985556\n",
            "iter 77 loss is 8.985355\n",
            "iter 78 loss is 8.985355\n",
            "iter 79 loss is 8.985350\n",
            "iter 80 loss is 8.985081\n",
            "iter 81 loss is 8.984916\n",
            "iter 82 loss is 8.984828\n",
            "iter 83 loss is 8.984686\n",
            "iter 84 loss is 8.984493\n",
            "iter 85 loss is 8.984445\n",
            "iter 86 loss is 8.984445\n",
            "iter 87 loss is 8.984360\n",
            "iter 88 loss is 8.984075\n",
            "iter 89 loss is 8.983859\n",
            "iter 90 loss is 8.983859\n",
            "iter 91 loss is 8.983859\n",
            "iter 92 loss is 8.983745\n",
            "iter 93 loss is 8.983493\n",
            "iter 94 loss is 8.982991\n",
            "iter 95 loss is 8.982991\n",
            "iter 96 loss is 8.982991\n",
            "iter 97 loss is 8.982991\n",
            "iter 98 loss is 8.982991\n",
            "iter 99 loss is 8.982991\n",
            "iter 100 loss is 8.982398\n",
            "iter 101 loss is 8.981949\n",
            "iter 102 loss is 8.981949\n",
            "iter 103 loss is 8.981949\n",
            "iter 104 loss is 8.981949\n",
            "iter 105 loss is 8.981949\n",
            "iter 106 loss is 8.981949\n",
            "iter 107 loss is 8.981726\n",
            "iter 108 loss is 8.981500\n",
            "iter 109 loss is 8.981500\n",
            "iter 110 loss is 8.981500\n",
            "iter 111 loss is 8.981039\n",
            "iter 112 loss is 8.981039\n",
            "iter 113 loss is 8.981039\n",
            "iter 114 loss is 8.981039\n",
            "iter 115 loss is 8.980816\n",
            "iter 116 loss is 8.980698\n",
            "iter 117 loss is 8.980698\n",
            "iter 118 loss is 8.980698\n",
            "iter 119 loss is 8.980609\n",
            "iter 120 loss is 8.980448\n",
            "iter 121 loss is 8.980448\n",
            "iter 122 loss is 8.980295\n",
            "iter 123 loss is 8.980295\n",
            "iter 124 loss is 8.979694\n",
            "iter 125 loss is 8.979694\n",
            "iter 126 loss is 8.979694\n",
            "iter 127 loss is 8.979694\n",
            "iter 128 loss is 8.979530\n",
            "iter 129 loss is 8.979435\n",
            "iter 130 loss is 8.979117\n",
            "iter 131 loss is 8.979117\n",
            "iter 132 loss is 8.979104\n",
            "iter 133 loss is 8.978982\n",
            "iter 134 loss is 8.978778\n",
            "iter 135 loss is 8.978778\n",
            "iter 136 loss is 8.978752\n",
            "iter 137 loss is 8.978752\n",
            "iter 138 loss is 8.978690\n",
            "iter 139 loss is 8.978690\n",
            "iter 140 loss is 8.978667\n",
            "iter 141 loss is 8.978667\n",
            "iter 142 loss is 8.978586\n",
            "iter 143 loss is 8.978403\n",
            "iter 144 loss is 8.978403\n",
            "iter 145 loss is 8.978403\n",
            "iter 146 loss is 8.978403\n",
            "iter 147 loss is 8.978403\n",
            "iter 148 loss is 8.978403\n",
            "iter 149 loss is 8.978364\n",
            "iter 150 loss is 8.978364\n",
            "iter 151 loss is 8.978322\n",
            "iter 152 loss is 8.977824\n",
            "iter 153 loss is 8.977738\n",
            "iter 154 loss is 8.977738\n",
            "iter 155 loss is 8.977585\n",
            "iter 156 loss is 8.977517\n",
            "iter 157 loss is 8.977517\n",
            "iter 158 loss is 8.977517\n",
            "iter 159 loss is 8.977317\n",
            "iter 160 loss is 8.977317\n",
            "iter 161 loss is 8.977317\n",
            "iter 162 loss is 8.977317\n",
            "iter 163 loss is 8.977042\n",
            "iter 164 loss is 8.976967\n",
            "iter 165 loss is 8.976666\n",
            "iter 166 loss is 8.976666\n",
            "iter 167 loss is 8.976666\n",
            "iter 168 loss is 8.976564\n",
            "iter 169 loss is 8.976304\n",
            "iter 170 loss is 8.976126\n",
            "iter 171 loss is 8.975980\n",
            "iter 172 loss is 8.975980\n",
            "iter 173 loss is 8.975980\n",
            "iter 174 loss is 8.975980\n",
            "iter 175 loss is 8.975899\n",
            "iter 176 loss is 8.975696\n",
            "iter 177 loss is 8.975652\n",
            "iter 178 loss is 8.975620\n",
            "iter 179 loss is 8.975506\n",
            "iter 180 loss is 8.975439\n",
            "iter 181 loss is 8.975439\n",
            "iter 182 loss is 8.975393\n",
            "iter 183 loss is 8.975344\n",
            "iter 184 loss is 8.975344\n",
            "iter 185 loss is 8.974908\n",
            "iter 186 loss is 8.974839\n",
            "iter 187 loss is 8.974678\n",
            "iter 188 loss is 8.974200\n",
            "iter 189 loss is 8.974066\n",
            "iter 190 loss is 8.973882\n",
            "iter 191 loss is 8.973882\n",
            "iter 192 loss is 8.973600\n",
            "iter 193 loss is 8.973498\n",
            "iter 194 loss is 8.973498\n",
            "iter 195 loss is 8.973360\n",
            "iter 196 loss is 8.973260\n",
            "iter 197 loss is 8.973149\n",
            "iter 198 loss is 8.972991\n",
            "iter 199 loss is 8.972991\n",
            "iter 200 loss is 8.972991\n",
            "iter 201 loss is 8.972890\n",
            "iter 202 loss is 8.972890\n",
            "iter 203 loss is 8.972834\n",
            "iter 204 loss is 8.972726\n",
            "iter 205 loss is 8.972675\n",
            "iter 206 loss is 8.972675\n",
            "iter 207 loss is 8.972380\n",
            "iter 208 loss is 8.972380\n",
            "iter 209 loss is 8.972216\n",
            "iter 210 loss is 8.972216\n",
            "iter 211 loss is 8.972216\n",
            "iter 212 loss is 8.971928\n",
            "iter 213 loss is 8.971927\n",
            "iter 214 loss is 8.971927\n",
            "iter 215 loss is 8.971862\n",
            "iter 216 loss is 8.971691\n",
            "iter 217 loss is 8.971691\n",
            "iter 218 loss is 8.971691\n",
            "iter 219 loss is 8.971287\n",
            "iter 220 loss is 8.971235\n",
            "iter 221 loss is 8.971235\n",
            "iter 222 loss is 8.971235\n",
            "iter 223 loss is 8.971128\n",
            "iter 224 loss is 8.971128\n",
            "iter 225 loss is 8.970980\n",
            "iter 226 loss is 8.970980\n",
            "iter 227 loss is 8.970743\n",
            "iter 228 loss is 8.970629\n",
            "iter 229 loss is 8.970364\n",
            "iter 230 loss is 8.970364\n",
            "iter 231 loss is 8.970085\n",
            "iter 232 loss is 8.970085\n",
            "iter 233 loss is 8.970073\n",
            "iter 234 loss is 8.970073\n",
            "iter 235 loss is 8.969853\n",
            "iter 236 loss is 8.969774\n",
            "iter 237 loss is 8.969636\n",
            "iter 238 loss is 8.969636\n",
            "iter 239 loss is 8.969504\n",
            "iter 240 loss is 8.969476\n",
            "iter 241 loss is 8.969476\n",
            "iter 242 loss is 8.968994\n",
            "iter 243 loss is 8.968830\n",
            "iter 244 loss is 8.968830\n",
            "iter 245 loss is 8.968692\n",
            "iter 246 loss is 8.968692\n",
            "iter 247 loss is 8.968463\n",
            "iter 248 loss is 8.968093\n",
            "iter 249 loss is 8.967840\n",
            "iter 250 loss is 8.967840\n",
            "iter 251 loss is 8.967840\n",
            "iter 252 loss is 8.967840\n",
            "iter 253 loss is 8.967728\n",
            "iter 254 loss is 8.967728\n",
            "iter 255 loss is 8.967169\n",
            "iter 256 loss is 8.967169\n",
            "iter 257 loss is 8.967169\n",
            "iter 258 loss is 8.966933\n",
            "iter 259 loss is 8.966700\n",
            "iter 260 loss is 8.966465\n",
            "iter 261 loss is 8.966465\n",
            "iter 262 loss is 8.966199\n",
            "iter 263 loss is 8.966005\n",
            "iter 264 loss is 8.965942\n",
            "iter 265 loss is 8.965470\n",
            "iter 266 loss is 8.965139\n",
            "iter 267 loss is 8.965139\n",
            "iter 268 loss is 8.965114\n",
            "iter 269 loss is 8.965114\n",
            "iter 270 loss is 8.965114\n",
            "iter 271 loss is 8.965026\n",
            "iter 272 loss is 8.965026\n",
            "iter 273 loss is 8.965026\n",
            "iter 274 loss is 8.964873\n",
            "iter 275 loss is 8.964738\n",
            "iter 276 loss is 8.964684\n",
            "iter 277 loss is 8.964677\n",
            "iter 278 loss is 8.964259\n",
            "iter 279 loss is 8.964213\n",
            "iter 280 loss is 8.964213\n",
            "iter 281 loss is 8.963946\n",
            "iter 282 loss is 8.963547\n",
            "iter 283 loss is 8.963264\n",
            "iter 284 loss is 8.963264\n",
            "iter 285 loss is 8.963030\n",
            "iter 286 loss is 8.963030\n",
            "iter 287 loss is 8.963030\n",
            "iter 288 loss is 8.963030\n",
            "iter 289 loss is 8.963030\n",
            "iter 290 loss is 8.962939\n",
            "iter 291 loss is 8.962939\n",
            "iter 292 loss is 8.962939\n",
            "iter 293 loss is 8.962939\n",
            "iter 294 loss is 8.962772\n",
            "iter 295 loss is 8.962772\n",
            "iter 296 loss is 8.962772\n",
            "iter 297 loss is 8.962737\n",
            "iter 298 loss is 8.962737\n",
            "iter 299 loss is 8.962547\n",
            "iter 300 loss is 8.962531\n",
            "iter 301 loss is 8.962474\n",
            "iter 302 loss is 8.962474\n",
            "iter 303 loss is 8.962474\n",
            "iter 304 loss is 8.962049\n",
            "iter 305 loss is 8.961567\n",
            "iter 306 loss is 8.961344\n",
            "iter 307 loss is 8.961344\n",
            "iter 308 loss is 8.961344\n",
            "iter 309 loss is 8.961232\n",
            "iter 310 loss is 8.961232\n",
            "iter 311 loss is 8.961213\n",
            "iter 312 loss is 8.961213\n",
            "iter 313 loss is 8.961213\n",
            "iter 314 loss is 8.961213\n",
            "iter 315 loss is 8.961213\n",
            "iter 316 loss is 8.961213\n",
            "iter 317 loss is 8.961206\n",
            "iter 318 loss is 8.961206\n",
            "iter 319 loss is 8.961206\n",
            "iter 320 loss is 8.961020\n",
            "iter 321 loss is 8.960449\n",
            "iter 322 loss is 8.960347\n",
            "iter 323 loss is 8.960347\n",
            "iter 324 loss is 8.960347\n",
            "iter 325 loss is 8.960122\n",
            "iter 326 loss is 8.960002\n",
            "iter 327 loss is 8.960002\n",
            "iter 328 loss is 8.960002\n",
            "iter 329 loss is 8.959768\n",
            "iter 330 loss is 8.959630\n",
            "iter 331 loss is 8.959439\n",
            "iter 332 loss is 8.959027\n",
            "iter 333 loss is 8.959027\n",
            "iter 334 loss is 8.958941\n",
            "iter 335 loss is 8.958908\n",
            "iter 336 loss is 8.958673\n",
            "iter 337 loss is 8.958197\n",
            "iter 338 loss is 8.958034\n",
            "iter 339 loss is 8.958034\n",
            "iter 340 loss is 8.958034\n",
            "iter 341 loss is 8.957862\n",
            "iter 342 loss is 8.957862\n",
            "iter 343 loss is 8.957862\n",
            "iter 344 loss is 8.957621\n",
            "iter 345 loss is 8.957621\n",
            "iter 346 loss is 8.957340\n",
            "iter 347 loss is 8.957340\n",
            "iter 348 loss is 8.956793\n",
            "iter 349 loss is 8.956713\n",
            "iter 350 loss is 8.956580\n",
            "iter 351 loss is 8.956575\n",
            "iter 352 loss is 8.956257\n",
            "iter 353 loss is 8.956257\n",
            "iter 354 loss is 8.956168\n",
            "iter 355 loss is 8.956160\n",
            "iter 356 loss is 8.956160\n",
            "iter 357 loss is 8.956127\n",
            "iter 358 loss is 8.956127\n",
            "iter 359 loss is 8.956127\n",
            "iter 360 loss is 8.956127\n",
            "iter 361 loss is 8.956127\n",
            "iter 362 loss is 8.956127\n",
            "iter 363 loss is 8.955851\n",
            "iter 364 loss is 8.955575\n",
            "iter 365 loss is 8.955268\n",
            "iter 366 loss is 8.954880\n",
            "iter 367 loss is 8.954880\n",
            "iter 368 loss is 8.954835\n",
            "iter 369 loss is 8.954824\n",
            "iter 370 loss is 8.954238\n",
            "iter 371 loss is 8.953948\n",
            "iter 372 loss is 8.953505\n",
            "iter 373 loss is 8.953346\n",
            "iter 374 loss is 8.953111\n",
            "iter 375 loss is 8.953111\n",
            "iter 376 loss is 8.952949\n",
            "iter 377 loss is 8.952723\n",
            "iter 378 loss is 8.952722\n",
            "iter 379 loss is 8.952722\n",
            "iter 380 loss is 8.952258\n",
            "iter 381 loss is 8.952228\n",
            "iter 382 loss is 8.952117\n",
            "iter 383 loss is 8.952117\n",
            "iter 384 loss is 8.951885\n",
            "iter 385 loss is 8.951885\n",
            "iter 386 loss is 8.951810\n",
            "iter 387 loss is 8.951810\n",
            "iter 388 loss is 8.951689\n",
            "iter 389 loss is 8.951550\n",
            "iter 390 loss is 8.951550\n",
            "iter 391 loss is 8.951550\n",
            "iter 392 loss is 8.951550\n",
            "iter 393 loss is 8.951550\n",
            "iter 394 loss is 8.951392\n",
            "iter 395 loss is 8.951392\n",
            "iter 396 loss is 8.951310\n",
            "iter 397 loss is 8.951152\n",
            "iter 398 loss is 8.951152\n",
            "iter 399 loss is 8.951118\n",
            "iter 400 loss is 8.951007\n",
            "iter 401 loss is 8.951007\n",
            "iter 402 loss is 8.951007\n",
            "iter 403 loss is 8.951007\n",
            "iter 404 loss is 8.950752\n",
            "iter 405 loss is 8.950752\n",
            "iter 406 loss is 8.950581\n",
            "iter 407 loss is 8.950546\n",
            "iter 408 loss is 8.950546\n",
            "iter 409 loss is 8.950543\n",
            "iter 410 loss is 8.950543\n",
            "iter 411 loss is 8.950541\n",
            "iter 412 loss is 8.950541\n",
            "iter 413 loss is 8.950294\n",
            "iter 414 loss is 8.950218\n",
            "iter 415 loss is 8.950217\n",
            "iter 416 loss is 8.949762\n",
            "iter 417 loss is 8.949501\n",
            "iter 418 loss is 8.949501\n",
            "iter 419 loss is 8.949501\n",
            "iter 420 loss is 8.949414\n",
            "iter 421 loss is 8.949277\n",
            "iter 422 loss is 8.949209\n",
            "iter 423 loss is 8.948773\n",
            "iter 424 loss is 8.948595\n",
            "iter 425 loss is 8.948377\n",
            "iter 426 loss is 8.947817\n",
            "iter 427 loss is 8.947817\n",
            "iter 428 loss is 8.947817\n",
            "iter 429 loss is 8.947817\n",
            "iter 430 loss is 8.947817\n",
            "iter 431 loss is 8.947765\n",
            "iter 432 loss is 8.947765\n",
            "iter 433 loss is 8.947765\n",
            "iter 434 loss is 8.947765\n",
            "iter 435 loss is 8.947671\n",
            "iter 436 loss is 8.947671\n",
            "iter 437 loss is 8.947541\n",
            "iter 438 loss is 8.947375\n",
            "iter 439 loss is 8.947375\n",
            "iter 440 loss is 8.947375\n",
            "iter 441 loss is 8.947282\n",
            "iter 442 loss is 8.946869\n",
            "iter 443 loss is 8.946583\n",
            "iter 444 loss is 8.946583\n",
            "iter 445 loss is 8.946583\n",
            "iter 446 loss is 8.946583\n",
            "iter 447 loss is 8.946581\n",
            "iter 448 loss is 8.946257\n",
            "iter 449 loss is 8.945957\n",
            "iter 450 loss is 8.945957\n",
            "iter 451 loss is 8.945713\n",
            "iter 452 loss is 8.945713\n",
            "iter 453 loss is 8.945613\n",
            "iter 454 loss is 8.945613\n",
            "iter 455 loss is 8.945583\n",
            "iter 456 loss is 8.945583\n",
            "iter 457 loss is 8.945436\n",
            "iter 458 loss is 8.945323\n",
            "iter 459 loss is 8.945323\n",
            "iter 460 loss is 8.945323\n",
            "iter 461 loss is 8.945132\n",
            "iter 462 loss is 8.945028\n",
            "iter 463 loss is 8.944665\n",
            "iter 464 loss is 8.944665\n",
            "iter 465 loss is 8.944618\n",
            "iter 466 loss is 8.944480\n",
            "iter 467 loss is 8.944153\n",
            "iter 468 loss is 8.944041\n",
            "iter 469 loss is 8.944034\n",
            "iter 470 loss is 8.944034\n",
            "iter 471 loss is 8.944034\n",
            "iter 472 loss is 8.944017\n",
            "iter 473 loss is 8.944017\n",
            "iter 474 loss is 8.943846\n",
            "iter 475 loss is 8.943563\n",
            "iter 476 loss is 8.943563\n",
            "iter 477 loss is 8.943563\n",
            "iter 478 loss is 8.943563\n",
            "iter 479 loss is 8.943433\n",
            "iter 480 loss is 8.943426\n",
            "iter 481 loss is 8.943368\n",
            "iter 482 loss is 8.943088\n",
            "iter 483 loss is 8.942985\n",
            "iter 484 loss is 8.942985\n",
            "iter 485 loss is 8.942814\n",
            "iter 486 loss is 8.942814\n",
            "iter 487 loss is 8.942814\n",
            "iter 488 loss is 8.942814\n",
            "iter 489 loss is 8.942814\n",
            "iter 490 loss is 8.942814\n",
            "iter 491 loss is 8.942793\n",
            "iter 492 loss is 8.942793\n",
            "iter 493 loss is 8.942793\n",
            "iter 494 loss is 8.942534\n",
            "iter 495 loss is 8.942448\n",
            "iter 496 loss is 8.942448\n",
            "iter 497 loss is 8.942448\n",
            "iter 498 loss is 8.942448\n",
            "iter 499 loss is 8.942327\n",
            "iter 500 loss is 8.942327\n",
            "iter 501 loss is 8.942327\n",
            "iter 502 loss is 8.942122\n",
            "iter 503 loss is 8.942122\n",
            "iter 504 loss is 8.942027\n",
            "iter 505 loss is 8.942027\n",
            "iter 506 loss is 8.941963\n",
            "iter 507 loss is 8.941768\n",
            "iter 508 loss is 8.941768\n",
            "iter 509 loss is 8.941768\n",
            "iter 510 loss is 8.941270\n",
            "iter 511 loss is 8.941108\n",
            "iter 512 loss is 8.940995\n",
            "iter 513 loss is 8.940995\n",
            "iter 514 loss is 8.940793\n",
            "iter 515 loss is 8.940793\n",
            "iter 516 loss is 8.940793\n",
            "iter 517 loss is 8.940793\n",
            "iter 518 loss is 8.940609\n",
            "iter 519 loss is 8.940609\n",
            "iter 520 loss is 8.940542\n",
            "iter 521 loss is 8.940542\n",
            "iter 522 loss is 8.940252\n",
            "iter 523 loss is 8.940252\n",
            "iter 524 loss is 8.940145\n",
            "iter 525 loss is 8.940145\n",
            "iter 526 loss is 8.939946\n",
            "iter 527 loss is 8.939699\n",
            "iter 528 loss is 8.939699\n",
            "iter 529 loss is 8.939489\n",
            "iter 530 loss is 8.939489\n",
            "iter 531 loss is 8.939264\n",
            "iter 532 loss is 8.939199\n",
            "iter 533 loss is 8.939134\n",
            "iter 534 loss is 8.939128\n",
            "iter 535 loss is 8.939040\n",
            "iter 536 loss is 8.938897\n",
            "iter 537 loss is 8.938774\n",
            "iter 538 loss is 8.938774\n",
            "iter 539 loss is 8.938683\n",
            "iter 540 loss is 8.938660\n",
            "iter 541 loss is 8.938660\n",
            "iter 542 loss is 8.938643\n",
            "iter 543 loss is 8.938643\n",
            "iter 544 loss is 8.938498\n",
            "iter 545 loss is 8.938498\n",
            "iter 546 loss is 8.938498\n",
            "iter 547 loss is 8.938267\n",
            "iter 548 loss is 8.938267\n",
            "iter 549 loss is 8.938203\n",
            "iter 550 loss is 8.937992\n",
            "iter 551 loss is 8.937664\n",
            "iter 552 loss is 8.937664\n",
            "iter 553 loss is 8.937664\n",
            "iter 554 loss is 8.937664\n",
            "iter 555 loss is 8.937402\n",
            "iter 556 loss is 8.936929\n",
            "iter 557 loss is 8.936658\n",
            "iter 558 loss is 8.936658\n",
            "iter 559 loss is 8.936392\n",
            "iter 560 loss is 8.936392\n",
            "iter 561 loss is 8.936345\n",
            "iter 562 loss is 8.935993\n",
            "iter 563 loss is 8.935993\n",
            "iter 564 loss is 8.935809\n",
            "iter 565 loss is 8.935809\n",
            "iter 566 loss is 8.935645\n",
            "iter 567 loss is 8.935645\n",
            "iter 568 loss is 8.935645\n",
            "iter 569 loss is 8.935645\n",
            "iter 570 loss is 8.935645\n",
            "iter 571 loss is 8.935543\n",
            "iter 572 loss is 8.935389\n",
            "iter 573 loss is 8.935389\n",
            "iter 574 loss is 8.935389\n",
            "iter 575 loss is 8.935389\n",
            "iter 576 loss is 8.935389\n",
            "iter 577 loss is 8.935389\n",
            "iter 578 loss is 8.935389\n",
            "iter 579 loss is 8.935389\n",
            "iter 580 loss is 8.935155\n",
            "iter 581 loss is 8.935155\n",
            "iter 582 loss is 8.934711\n",
            "iter 583 loss is 8.934635\n",
            "iter 584 loss is 8.934635\n",
            "iter 585 loss is 8.934537\n",
            "iter 586 loss is 8.934537\n",
            "iter 587 loss is 8.934489\n",
            "iter 588 loss is 8.934489\n",
            "iter 589 loss is 8.934489\n",
            "iter 590 loss is 8.934489\n",
            "iter 591 loss is 8.934489\n",
            "iter 592 loss is 8.934489\n",
            "iter 593 loss is 8.934489\n",
            "iter 594 loss is 8.934489\n",
            "iter 595 loss is 8.934489\n",
            "iter 596 loss is 8.934405\n",
            "iter 597 loss is 8.934405\n",
            "iter 598 loss is 8.934405\n",
            "iter 599 loss is 8.934405\n",
            "iter 600 loss is 8.934405\n",
            "iter 601 loss is 8.934405\n",
            "iter 602 loss is 8.934405\n",
            "iter 603 loss is 8.934405\n",
            "iter 604 loss is 8.934377\n",
            "iter 605 loss is 8.934377\n",
            "iter 606 loss is 8.934301\n",
            "iter 607 loss is 8.934301\n",
            "iter 608 loss is 8.934292\n",
            "iter 609 loss is 8.934252\n",
            "iter 610 loss is 8.934252\n",
            "iter 611 loss is 8.934021\n",
            "iter 612 loss is 8.933595\n",
            "iter 613 loss is 8.933595\n",
            "iter 614 loss is 8.933595\n",
            "iter 615 loss is 8.933424\n",
            "iter 616 loss is 8.933424\n",
            "iter 617 loss is 8.933424\n",
            "iter 618 loss is 8.933349\n",
            "iter 619 loss is 8.933349\n",
            "iter 620 loss is 8.933264\n",
            "iter 621 loss is 8.933264\n",
            "iter 622 loss is 8.933128\n",
            "iter 623 loss is 8.933128\n",
            "iter 624 loss is 8.933128\n",
            "iter 625 loss is 8.932850\n",
            "iter 626 loss is 8.932850\n",
            "iter 627 loss is 8.932809\n",
            "iter 628 loss is 8.932809\n",
            "iter 629 loss is 8.932809\n",
            "iter 630 loss is 8.932809\n",
            "iter 631 loss is 8.932809\n",
            "iter 632 loss is 8.932809\n",
            "iter 633 loss is 8.932581\n",
            "iter 634 loss is 8.932581\n",
            "iter 635 loss is 8.932581\n",
            "iter 636 loss is 8.932058\n",
            "iter 637 loss is 8.931903\n",
            "iter 638 loss is 8.931899\n",
            "iter 639 loss is 8.931899\n",
            "iter 640 loss is 8.931899\n",
            "iter 641 loss is 8.931899\n",
            "iter 642 loss is 8.931899\n",
            "iter 643 loss is 8.931877\n",
            "iter 644 loss is 8.931665\n",
            "iter 645 loss is 8.931665\n",
            "iter 646 loss is 8.931283\n",
            "iter 647 loss is 8.931283\n",
            "iter 648 loss is 8.931010\n",
            "iter 649 loss is 8.931010\n",
            "iter 650 loss is 8.931010\n",
            "iter 651 loss is 8.931010\n",
            "iter 652 loss is 8.931010\n",
            "iter 653 loss is 8.931010\n",
            "iter 654 loss is 8.930593\n",
            "iter 655 loss is 8.930422\n",
            "iter 656 loss is 8.930357\n",
            "iter 657 loss is 8.930357\n",
            "iter 658 loss is 8.930357\n",
            "iter 659 loss is 8.930357\n",
            "iter 660 loss is 8.930293\n",
            "iter 661 loss is 8.930293\n",
            "iter 662 loss is 8.929931\n",
            "iter 663 loss is 8.929673\n",
            "iter 664 loss is 8.929673\n",
            "iter 665 loss is 8.929673\n",
            "iter 666 loss is 8.929333\n",
            "iter 667 loss is 8.929237\n",
            "iter 668 loss is 8.929237\n",
            "iter 669 loss is 8.929237\n",
            "iter 670 loss is 8.929237\n",
            "iter 671 loss is 8.929237\n",
            "iter 672 loss is 8.929237\n",
            "iter 673 loss is 8.929237\n",
            "iter 674 loss is 8.928791\n",
            "iter 675 loss is 8.928602\n",
            "iter 676 loss is 8.928602\n",
            "iter 677 loss is 8.928587\n",
            "iter 678 loss is 8.928587\n",
            "iter 679 loss is 8.928587\n",
            "iter 680 loss is 8.928473\n",
            "iter 681 loss is 8.928473\n",
            "iter 682 loss is 8.928471\n",
            "iter 683 loss is 8.928106\n",
            "iter 684 loss is 8.928016\n",
            "iter 685 loss is 8.927764\n",
            "iter 686 loss is 8.927710\n",
            "iter 687 loss is 8.927696\n",
            "iter 688 loss is 8.927561\n",
            "iter 689 loss is 8.927472\n",
            "iter 690 loss is 8.927163\n",
            "iter 691 loss is 8.927163\n",
            "iter 692 loss is 8.927163\n",
            "iter 693 loss is 8.926839\n",
            "iter 694 loss is 8.926839\n",
            "iter 695 loss is 8.926839\n",
            "iter 696 loss is 8.926839\n",
            "iter 697 loss is 8.926544\n",
            "iter 698 loss is 8.926544\n",
            "iter 699 loss is 8.926385\n",
            "iter 700 loss is 8.926385\n",
            "iter 701 loss is 8.926329\n",
            "iter 702 loss is 8.925945\n",
            "iter 703 loss is 8.925945\n",
            "iter 704 loss is 8.925945\n",
            "iter 705 loss is 8.925945\n",
            "iter 706 loss is 8.925945\n",
            "iter 707 loss is 8.925779\n",
            "iter 708 loss is 8.925779\n",
            "iter 709 loss is 8.925779\n",
            "iter 710 loss is 8.925779\n",
            "iter 711 loss is 8.925381\n",
            "iter 712 loss is 8.925242\n",
            "iter 713 loss is 8.925242\n",
            "iter 714 loss is 8.925242\n",
            "iter 715 loss is 8.925242\n",
            "iter 716 loss is 8.925185\n",
            "iter 717 loss is 8.925033\n",
            "iter 718 loss is 8.924827\n",
            "iter 719 loss is 8.924827\n",
            "iter 720 loss is 8.924814\n",
            "iter 721 loss is 8.924814\n",
            "iter 722 loss is 8.924570\n",
            "iter 723 loss is 8.924570\n",
            "iter 724 loss is 8.924560\n",
            "iter 725 loss is 8.924335\n",
            "iter 726 loss is 8.924335\n",
            "iter 727 loss is 8.924281\n",
            "iter 728 loss is 8.924281\n",
            "iter 729 loss is 8.923985\n",
            "iter 730 loss is 8.923854\n",
            "iter 731 loss is 8.923837\n",
            "iter 732 loss is 8.923837\n",
            "iter 733 loss is 8.923837\n",
            "iter 734 loss is 8.923573\n",
            "iter 735 loss is 8.923399\n",
            "iter 736 loss is 8.923399\n",
            "iter 737 loss is 8.923266\n",
            "iter 738 loss is 8.922956\n",
            "iter 739 loss is 8.922956\n",
            "iter 740 loss is 8.922956\n",
            "iter 741 loss is 8.922882\n",
            "iter 742 loss is 8.922882\n",
            "iter 743 loss is 8.922772\n",
            "iter 744 loss is 8.922574\n",
            "iter 745 loss is 8.922535\n",
            "iter 746 loss is 8.922497\n",
            "iter 747 loss is 8.922497\n",
            "iter 748 loss is 8.922497\n",
            "iter 749 loss is 8.922474\n",
            "iter 750 loss is 8.922351\n",
            "iter 751 loss is 8.922284\n",
            "iter 752 loss is 8.922284\n",
            "iter 753 loss is 8.922069\n",
            "iter 754 loss is 8.921616\n",
            "iter 755 loss is 8.921616\n",
            "iter 756 loss is 8.921525\n",
            "iter 757 loss is 8.921525\n",
            "iter 758 loss is 8.921525\n",
            "iter 759 loss is 8.921444\n",
            "iter 760 loss is 8.921444\n",
            "iter 761 loss is 8.921302\n",
            "iter 762 loss is 8.920964\n",
            "iter 763 loss is 8.920776\n",
            "iter 764 loss is 8.920682\n",
            "iter 765 loss is 8.920682\n",
            "iter 766 loss is 8.920682\n",
            "iter 767 loss is 8.920221\n",
            "iter 768 loss is 8.920221\n",
            "iter 769 loss is 8.920221\n",
            "iter 770 loss is 8.920221\n",
            "iter 771 loss is 8.920221\n",
            "iter 772 loss is 8.920083\n",
            "iter 773 loss is 8.919982\n",
            "iter 774 loss is 8.919920\n",
            "iter 775 loss is 8.919920\n",
            "iter 776 loss is 8.919920\n",
            "iter 777 loss is 8.919639\n",
            "iter 778 loss is 8.919609\n",
            "iter 779 loss is 8.919279\n",
            "iter 780 loss is 8.919106\n",
            "iter 781 loss is 8.918884\n",
            "iter 782 loss is 8.918884\n",
            "iter 783 loss is 8.918884\n",
            "iter 784 loss is 8.918884\n",
            "iter 785 loss is 8.918884\n",
            "iter 786 loss is 8.918866\n",
            "iter 787 loss is 8.918866\n",
            "iter 788 loss is 8.918866\n",
            "iter 789 loss is 8.918866\n",
            "iter 790 loss is 8.918866\n",
            "iter 791 loss is 8.918453\n",
            "iter 792 loss is 8.918244\n",
            "iter 793 loss is 8.918146\n",
            "iter 794 loss is 8.918031\n",
            "iter 795 loss is 8.918031\n",
            "iter 796 loss is 8.918031\n",
            "iter 797 loss is 8.917920\n",
            "iter 798 loss is 8.917571\n",
            "iter 799 loss is 8.917302\n",
            "iter 800 loss is 8.917164\n",
            "iter 801 loss is 8.917164\n",
            "iter 802 loss is 8.916961\n",
            "iter 803 loss is 8.916513\n",
            "iter 804 loss is 8.916204\n",
            "iter 805 loss is 8.916204\n",
            "iter 806 loss is 8.916204\n",
            "iter 807 loss is 8.915874\n",
            "iter 808 loss is 8.915874\n",
            "iter 809 loss is 8.915874\n",
            "iter 810 loss is 8.915530\n",
            "iter 811 loss is 8.915495\n",
            "iter 812 loss is 8.915495\n",
            "iter 813 loss is 8.915297\n",
            "iter 814 loss is 8.915282\n",
            "iter 815 loss is 8.915203\n",
            "iter 816 loss is 8.915203\n",
            "iter 817 loss is 8.915145\n",
            "iter 818 loss is 8.915145\n",
            "iter 819 loss is 8.915108\n",
            "iter 820 loss is 8.915108\n",
            "iter 821 loss is 8.914875\n",
            "iter 822 loss is 8.914875\n",
            "iter 823 loss is 8.914846\n",
            "iter 824 loss is 8.914846\n",
            "iter 825 loss is 8.914846\n",
            "iter 826 loss is 8.914817\n",
            "iter 827 loss is 8.914571\n",
            "iter 828 loss is 8.914571\n",
            "iter 829 loss is 8.914571\n",
            "iter 830 loss is 8.914571\n",
            "iter 831 loss is 8.914571\n",
            "iter 832 loss is 8.914291\n",
            "iter 833 loss is 8.914209\n",
            "iter 834 loss is 8.914069\n",
            "iter 835 loss is 8.914008\n",
            "iter 836 loss is 8.913829\n",
            "iter 837 loss is 8.913553\n",
            "iter 838 loss is 8.913163\n",
            "iter 839 loss is 8.913163\n",
            "iter 840 loss is 8.913163\n",
            "iter 841 loss is 8.913163\n",
            "iter 842 loss is 8.913034\n",
            "iter 843 loss is 8.912999\n",
            "iter 844 loss is 8.912999\n",
            "iter 845 loss is 8.912894\n",
            "iter 846 loss is 8.912866\n",
            "iter 847 loss is 8.912866\n",
            "iter 848 loss is 8.912763\n",
            "iter 849 loss is 8.912725\n",
            "iter 850 loss is 8.912630\n",
            "iter 851 loss is 8.912493\n",
            "iter 852 loss is 8.912293\n",
            "iter 853 loss is 8.912293\n",
            "iter 854 loss is 8.912293\n",
            "iter 855 loss is 8.912293\n",
            "iter 856 loss is 8.912293\n",
            "iter 857 loss is 8.912293\n",
            "iter 858 loss is 8.912293\n",
            "iter 859 loss is 8.912293\n",
            "iter 860 loss is 8.912293\n",
            "iter 861 loss is 8.912293\n",
            "iter 862 loss is 8.912293\n",
            "iter 863 loss is 8.911911\n",
            "iter 864 loss is 8.911732\n",
            "iter 865 loss is 8.911659\n",
            "iter 866 loss is 8.911659\n",
            "iter 867 loss is 8.911659\n",
            "iter 868 loss is 8.911129\n",
            "iter 869 loss is 8.910865\n",
            "iter 870 loss is 8.910865\n",
            "iter 871 loss is 8.910791\n",
            "iter 872 loss is 8.910791\n",
            "iter 873 loss is 8.910668\n",
            "iter 874 loss is 8.910668\n",
            "iter 875 loss is 8.910668\n",
            "iter 876 loss is 8.910361\n",
            "iter 877 loss is 8.910361\n",
            "iter 878 loss is 8.909877\n",
            "iter 879 loss is 8.909877\n",
            "iter 880 loss is 8.909662\n",
            "iter 881 loss is 8.909662\n",
            "iter 882 loss is 8.908822\n",
            "iter 883 loss is 8.908429\n",
            "iter 884 loss is 8.908244\n",
            "iter 885 loss is 8.907952\n",
            "iter 886 loss is 8.907666\n",
            "iter 887 loss is 8.907666\n",
            "iter 888 loss is 8.907666\n",
            "iter 889 loss is 8.907666\n",
            "iter 890 loss is 8.907666\n",
            "iter 891 loss is 8.907666\n",
            "iter 892 loss is 8.907653\n",
            "iter 893 loss is 8.907653\n",
            "iter 894 loss is 8.907175\n",
            "iter 895 loss is 8.907175\n",
            "iter 896 loss is 8.907057\n",
            "iter 897 loss is 8.906602\n",
            "iter 898 loss is 8.906602\n",
            "iter 899 loss is 8.906568\n",
            "iter 900 loss is 8.906379\n",
            "iter 901 loss is 8.906379\n",
            "iter 902 loss is 8.906242\n",
            "iter 903 loss is 8.906225\n",
            "iter 904 loss is 8.906225\n",
            "iter 905 loss is 8.906225\n",
            "iter 906 loss is 8.906159\n",
            "iter 907 loss is 8.905920\n",
            "iter 908 loss is 8.905840\n",
            "iter 909 loss is 8.905675\n",
            "iter 910 loss is 8.905675\n",
            "iter 911 loss is 8.905675\n",
            "iter 912 loss is 8.905611\n",
            "iter 913 loss is 8.905611\n",
            "iter 914 loss is 8.905249\n",
            "iter 915 loss is 8.905249\n",
            "iter 916 loss is 8.905249\n",
            "iter 917 loss is 8.905212\n",
            "iter 918 loss is 8.905212\n",
            "iter 919 loss is 8.905212\n",
            "iter 920 loss is 8.905212\n",
            "iter 921 loss is 8.905212\n",
            "iter 922 loss is 8.904996\n",
            "iter 923 loss is 8.904996\n",
            "iter 924 loss is 8.904996\n",
            "iter 925 loss is 8.904490\n",
            "iter 926 loss is 8.904412\n",
            "iter 927 loss is 8.904371\n",
            "iter 928 loss is 8.904371\n",
            "iter 929 loss is 8.904371\n",
            "iter 930 loss is 8.904371\n",
            "iter 931 loss is 8.904371\n",
            "iter 932 loss is 8.904126\n",
            "iter 933 loss is 8.903690\n",
            "iter 934 loss is 8.903690\n",
            "iter 935 loss is 8.903690\n",
            "iter 936 loss is 8.903427\n",
            "iter 937 loss is 8.903300\n",
            "iter 938 loss is 8.903300\n",
            "iter 939 loss is 8.902999\n",
            "iter 940 loss is 8.902999\n",
            "iter 941 loss is 8.902901\n",
            "iter 942 loss is 8.902860\n",
            "iter 943 loss is 8.902860\n",
            "iter 944 loss is 8.902860\n",
            "iter 945 loss is 8.902595\n",
            "iter 946 loss is 8.902595\n",
            "iter 947 loss is 8.902124\n",
            "iter 948 loss is 8.902000\n",
            "iter 949 loss is 8.902000\n",
            "iter 950 loss is 8.902000\n",
            "iter 951 loss is 8.901765\n",
            "iter 952 loss is 8.901461\n",
            "iter 953 loss is 8.901461\n",
            "iter 954 loss is 8.901461\n",
            "iter 955 loss is 8.901461\n",
            "iter 956 loss is 8.901461\n",
            "iter 957 loss is 8.901461\n",
            "iter 958 loss is 8.901061\n",
            "iter 959 loss is 8.901061\n",
            "iter 960 loss is 8.901032\n",
            "iter 961 loss is 8.901018\n",
            "iter 962 loss is 8.900908\n",
            "iter 963 loss is 8.900908\n",
            "iter 964 loss is 8.900908\n",
            "iter 965 loss is 8.900908\n",
            "iter 966 loss is 8.900364\n",
            "iter 967 loss is 8.900295\n",
            "iter 968 loss is 8.900241\n",
            "iter 969 loss is 8.899940\n",
            "iter 970 loss is 8.899940\n",
            "iter 971 loss is 8.899785\n",
            "iter 972 loss is 8.899785\n",
            "iter 973 loss is 8.899785\n",
            "iter 974 loss is 8.899758\n",
            "iter 975 loss is 8.899758\n",
            "iter 976 loss is 8.899758\n",
            "iter 977 loss is 8.899701\n",
            "iter 978 loss is 8.899640\n",
            "iter 979 loss is 8.899640\n",
            "iter 980 loss is 8.899279\n",
            "iter 981 loss is 8.898848\n",
            "iter 982 loss is 8.898848\n",
            "iter 983 loss is 8.898723\n",
            "iter 984 loss is 8.898723\n",
            "iter 985 loss is 8.898723\n",
            "iter 986 loss is 8.898508\n",
            "iter 987 loss is 8.898508\n",
            "iter 988 loss is 8.898508\n",
            "iter 989 loss is 8.898508\n",
            "iter 990 loss is 8.898143\n",
            "iter 991 loss is 8.898143\n",
            "iter 992 loss is 8.898136\n",
            "iter 993 loss is 8.898136\n",
            "iter 994 loss is 8.898110\n",
            "iter 995 loss is 8.897808\n",
            "iter 996 loss is 8.897712\n",
            "iter 997 loss is 8.897712\n",
            "iter 998 loss is 8.897712\n",
            "iter 999 loss is 8.897712\n"
          ]
        }
      ],
      "source": [
        "# Random Local Search\n",
        "W = np.random.randn(10, 3073) * 0.001\n",
        "bestloss = float(\"inf\")\n",
        "for i in range(1000):\n",
        "    step_size = 0.0001\n",
        "    Wtry = W + np.random.randn(10, 3073) * step_size\n",
        "    loss = L(X_train, Y_train, Wtry)\n",
        "    if loss < bestloss:\n",
        "        W = Wtry\n",
        "        bestloss = loss\n",
        "    print(f'iter {i} loss is {bestloss:.6f}')\n",
        "\n",
        "# Strategy 3: Gradient Numerical Approximation\n",
        "def eval_numerical_gradient(f, x):\n",
        "    fx = f(x)\n",
        "    grad = np.zeros(x.shape)\n",
        "    h = 1e-5\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "        old_value = x[ix]\n",
        "        x[ix] = old_value + h\n",
        "        fxh = f(x)\n",
        "        x[ix] = old_value\n",
        "        grad[ix] = (fxh - fx) / h\n",
        "        it.iternext()\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9lhc18Xg4cg9",
        "outputId": "a405e578-cc20-4a5f-e8eb-5ecc2d1894f5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CIFAR10_loss_fun' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3461324134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3073\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'original loss: {loss_original:.6f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CIFAR10_loss_fun' is not defined"
          ]
        }
      ],
      "source": [
        "W = np.random.rand(10, 3073) * 0.001\n",
        "df = eval_numerical_gradient(CIFAR10_loss_fun, W)\n",
        "\n",
        "loss_original = CIFAR10_loss_fun(W)\n",
        "print(f'original loss: {loss_original:.6f}')\n",
        "\n",
        "for step_size_log in range(-10, 0):\n",
        "    step_size = 10 ** step_size_log\n",
        "    W_new = W - step_size * df\n",
        "    loss_new = CIFAR10_loss_fun(W_new)\n",
        "    print(f'for step size {step_size:.9f} new loss: {loss_new:.6f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}